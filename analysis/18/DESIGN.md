# Evaluation Test Harness v2

## Problem / Metric

> Current evaluation tests scattered in implemented/pending folders without automated state tracking or progress visibility
> **Metric to move**: Transform ad-hoc evaluation management into git-native 5-state workflow with automated progress tracking

## Goal

Create a git-native evaluation harness that maintains analyst comfort (markdown/CSV workflow) while enabling automated state management and progress tracking for all 77 evaluation tests.

## Scope (M / S / W)

- **[M]** Five-folder structure (todo/review/passed/failed/blocked) with state transitions via git mv
- **[M]** Machine-readable headers for all test files with automation metadata
- **[M]** Migration plan to move 34 existing files to new structure
- **[M]** Automated test runner (`run_evals.py`) with state transitions
- **[S]** Progress dashboard generator (`progress.md`) for business visibility
- **[S]** Template system for creating new evaluation tests
- **[W]** Integration with external issue trackers or spreadsheet tools

## Acceptance Criteria

| # | Given | When | Then |
|---|-------|------|------|
| 1 | 34 existing evaluation files | Migration script runs | All moved to review/ folder with proper headers |
| 2 | Test file in todo/ folder | run_evals.py executes | File auto-moves to passed/failed based on result |
| 3 | Analyst opens progress.md | Dashboard generated | Shows current status of all 77 tests with timestamps |
| 4 | New test idea created | File dropped in todo/ | Follows template format with proper header |
| 5 | Business user needs status | Opens progress.md or exports to CSV | Clear visibility into evaluation progress |

## Architecture: Five-Folder State Machine

```
evals/
├── template.md          # blank test scaffolding
├── todo/                # test idea exists, never run
├── review/              # executed, needs human/AI verdict
├── passed/              # accepted – criteria met
├── failed/              # executed, criteria not met
└── blocked/             # deferred – feature/data missing
```

Moving a file between folders **is** the state transition (`git mv …`).
No external issue tracker required.

## Machine-Readable File Headers

```markdown
<!--- META: machine-readable for scripts --->
Status: TODO          # TODO | REVIEW | PASSED | FAILED | BLOCKED
ID:      EVAL-XX
Category: <one-word tag>   # e.g. Content, Alias, Analytics
Added:   2025-06-29
Last-Run: —           # auto-filled by runner (ISO timestamp)
Duration-ms: —        # auto (integer)
Run-Count: 0          # auto (increments each run)
Blocker: —            # short note or "—"
```

Everything after the header remains **human-oriented** (business goal, query, expected outcome, etc.).
Analysts can still paste table-like parts into Excel if they wish.

## State Transition Logic

```
todo  → review   # when first executed
review→ passed   # accepted
review→ failed   # analyst says "wrong"
review→ blocked  # can't be tested meaningfully yet
failed → review  # fix applied, retest needed
blocked→ review  # blocker resolved
passed ↔ blocked # feature removed / restored
```

*Jump directly todo→passed or failed→passed if automation can decide with high confidence.*

## Progress Dashboard (`progress.md`)

Generated by automated script:

| ID | Question            | Status | Last Run   | Dur    | Notes                    |
| -- | ------------------- | ------ | ---------- | ------ | ------------------------ |
| 08 | Sago palm mentions? | ✅      | 2025-06-29 | 120 ms | —                        |
| 12 | New phone detection | ❌      | 2025-06-29 | 95 ms  | wrong alias              |
| 22 | Topic clustering    | ⬜      | —          | —      | to do                    |
| 45 | SMS threading       | ⏸      | —          | —      | waiting on vector search |

Legend – ✅ passed, ❌ failed, 🟠 review, ⬜ todo, ⏸ blocked.
Business users can open this Markdown or paste it into Excel.

## Migration Plan

1. **Create folders + template**
   ```bash
   mkdir -p evals/{todo,review,passed,failed,blocked}
   cp docs/template.md evals/template.md
   ```

2. **Move current tests to `review/` (none had formal verdict yet)**
   ```bash
   git mv evals/implemented/* evals/review/
   git mv evals/pending/* evals/todo/
   ```

3. **Prepend headers** to every file (VS Code multi-cursor or automated script)

4. **Commit** – "Scaffold v2 evaluation harness; all tests migrated"

5. **Build automation**
   - `run_evals.py` runs tests in `todo/` + `review/`
   - Updates `Last-Run`, `Duration-ms`, `Run-Count`
   - Sets **Status** to PASSED/FAILED automatically
   - Moves files to matching folders (`git mv`)
   - Regenerates `progress.md`

6. **Validation**
   - Analyst spot-check FAILED + sample PASSED files
   - If PASSED file is wrong, analyst edits header and moves to `failed/`

7. **Future workflow**
   - New idea → drop file in `todo/` (Status: TODO)
   - Weekly/on-demand: run `run_evals.py`, commit results

## Business User Comfort

- Analysts still work in **Markdown** or exported **CSV** views
- No need to touch GitHub Issues; folders and scripts handle everything
- Progress dashboard can be pasted into shared spreadsheets if needed
- Familiar file-based workflow with git audit trail

## Implementation Phases

1. **Infrastructure**: Create folder structure, template, and basic migration
2. **Data Migration**: Move 34 existing files with headers
3. **Automation**: Build test runner with state transitions
4. **Reporting**: Implement progress dashboard generator

## Risks / Blockers / Owner

- **Risk**: Analyst workflow disruption → mitigate with training and gradual rollout
- **Risk**: Test execution automation complexity → start with manual verification
- **Owner**: Engineering team with analyst feedback loop

---

This creates a **fully traceable yet low-overhead** evaluation system with no external tooling, no spreadsheet duplication, and complete git-friendly workflow.